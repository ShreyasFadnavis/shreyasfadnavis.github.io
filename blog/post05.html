<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Leverage and Influence: The 50-Year-Old Tool That ML Forgot — Shreyas Fadnavis</title>
  <link rel="stylesheet" href="../css/style.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}]});"></script>
  <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
  <script>mermaid.initialize({startOnLoad: true, theme: 'base', themeVariables: {primaryColor: '#f3f0ec', primaryTextColor: '#1c1917', primaryBorderColor: '#a0522d', lineColor: '#a0522d', secondaryColor: '#faf8f5', tertiaryColor: '#e5e0da'}});</script>
</head>
<body>

<nav>
  <div class="nav-inner">
    <a href="/" class="site-name">Shreyas Fadnavis</a>
    <div class="nav-links">
      <a href="/">About</a>
      <a href="/blog/" class="active">Notes</a>
    </div>
  </div>
</nav>

<main>
  <article class="post">
    <div class="post-header">
      <h1>Leverage and Influence: The 50-Year-Old Tool That ML Forgot</h1>
      <p class="post-subtitle">Part 5 of a 10-part series on prediction intervals, conformal prediction, and leverage scores.</p>
    </div>

    <div class="series-banner">
      This is part of the series <a href="/blog/">From Predictions to Prediction Intervals</a>.
    </div>

    <div class="post-body">

      <p>In Part 4, we defined leverage scores as the diagonal of the hat matrix and showed that they measure how far each point sits from the center of the training data in Mahalanobis distance. In this post, we explore the classical use of leverage in regression diagnostics -- a rich tradition dating back to the late 1970s that the machine learning community has largely overlooked.</p>

      <p>The concepts here are classical but important: leverage, influence, and Cook's distance together form a diagnostic toolkit that provides a clear way to understand <em>which data points matter most</em> and <em>why</em>. We present this material at three levels of depth.</p>

      <!-- ============================================================
           LEVEL 1: INTUITIVE
           ============================================================ -->
      <div class="level">
        <div class="level-header" role="button" aria-expanded="true" onclick="this.parentElement.querySelector('.level-content').classList.toggle('collapsed'); this.setAttribute('aria-expanded', this.getAttribute('aria-expanded')==='true'?'false':'true');">
          <span class="level-badge intuitive">Intuitive</span>
          <h2>The Big Picture: Unusual Points and Their Power</h2>
          <span class="level-toggle" aria-hidden="true">&#9660;</span>
        </div>
        <div class="level-content">

          <h3>Two Types of Unusual Data Points</h3>

          <p>There are two distinct ways a data point can be "unusual," and it is worth separating them carefully. Confusing the two is a common source of error in regression diagnostics.</p>

          <p><strong>Type 1: The Leverage Point.</strong> This is a point that sits in an unusual <em>location</em> in feature space but has a normal outcome. It is "different" in where it lives, not in what it does. Think of it as someone who lives in a remote area but behaves like everyone else. Their <em>position</em> is unusual, but their <em>behavior</em> is not. In a regression setting, such a point lies far from the centroid of the training features, yet its response falls right on (or near) the fitted line.</p>

          <p><strong>Type 2: The Influential Point.</strong> This is a point that sits in an unusual location <em>and</em> has an unusual outcome. Because of its position, it has the ability to <em>change the fitted model</em>. Remove it, and the fitted coefficients shift noticeably. This is the case that warrants closer inspection. A concrete example: if a dataset of house prices has one mansion priced well below market value, that single data point can tilt the entire regression surface because there are few other data points nearby to counterbalance it.</p>

          <div class="analogy">
            <div class="analogy-label">Analogy: The City Council Vote</div>
            <p>Imagine a city council making a decision by majority vote. A council member representing a remote, sparsely populated district has <strong>high leverage</strong> -- their district is geographically unusual, giving them a unique position. If this remote council member votes with the majority (low residual), nothing changes. The decision would have been the same without them. But if this remote council member votes <em>against</em> everyone else (high leverage + high residual), they can swing the entire decision. <em>That</em> is influence.</p>
          </div>

          <h3>The 2x2 Grid</h3>

          <p>Every data point falls into one of four categories, based on two questions: Is its position unusual? Is its outcome unusual? The following table summarizes the four cases. In practice, most points land in the upper-left cell; the lower-right cell is the one that demands attention.</p>

          <table>
            <thead>
              <tr>
                <th></th>
                <th>Low Residual (normal outcome)</th>
                <th>High Residual (unusual outcome)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>Low Leverage</strong> (normal position)</td>
                <td><strong>Normal.</strong> Typical point, well-fit by the model. These make up the bulk of most datasets.</td>
                <td><strong>Outlier.</strong> Unusual response, but its position gives it little power to distort the fit. Worth noting, but typically harmless to the overall model.</td>
              </tr>
              <tr>
                <td><strong>High Leverage</strong> (unusual position)</td>
                <td><strong>Leverage Point.</strong> Unusual features, but the model fits it well. Potentially helpful -- it provides information in a sparse region.</td>
                <td><strong>Influential Point.</strong> Unusual features AND unusual response. This point pulls the fitted surface toward itself and merits careful examination.</td>
              </tr>
            </tbody>
          </table>

          <div class="mermaid">
          quadrantChart
            title Data Point Classification
            x-axis "Low Leverage" --> "High Leverage"
            y-axis "Low Residual" --> "High Residual"
            quadrant-1 "INFLUENTIAL (Investigate)"
            quadrant-2 "Outlier (Noisy)"
            quadrant-3 "Normal (Typical)"
            quadrant-4 "Leverage Point (Unusual but OK)"
          </div>
          <p class="diagram-caption">The four types of data points, classified by leverage (position unusualness) and residual (outcome unusualness).</p>

          <h3>Cook's Distance in Plain English</h3>

          <p><strong>Cook's distance</strong> answers a natural question: "If we removed this one data point and refitted the model, how much would all the predictions change?" It quantifies the total shift in the vector of fitted values when a single observation is left out.</p>

          <p>A point with high Cook's distance is one whose removal noticeably alters the fitted model. A point with low Cook's distance can be removed with negligible effect. What makes Cook's distance useful is that it captures <em>both</em> leverage and residual in a single number -- you need both an unusual position and an unusual outcome to have high influence. In practice, a common rule of thumb is that a Cook's distance exceeding $4/n$ deserves a closer look; values above 1 indicate that the point is shifting the fitted surface by roughly the width of a joint confidence region for the coefficients.</p>

          <h3>How a Data Point Gets Classified</h3>

          <div class="mermaid">
          flowchart TD
            A["Observe data point"] --> B["Compute leverage hᵢ"]
            B --> C["Compute residual eᵢ"]
            C --> D{"hᵢ > threshold?"}
            D -- "No" --> E{"Large |eᵢ|?"}
            D -- "Yes" --> F{"Large |eᵢ|?"}
            E -- "No" --> G["Normal Point"]
            E -- "Yes" --> H["Outlier"]
            F -- "No" --> I["Leverage Point"]
            F -- "Yes" --> J["INFLUENTIAL"]

            style G fill:#e8f5e9,stroke:#2e7d32,color:#1c1917
            style H fill:#fff3e0,stroke:#e65100,color:#1c1917
            style I fill:#e3f2fd,stroke:#1565c0,color:#1c1917
            style J fill:#fce4ec,stroke:#c62828,color:#1c1917
          </div>
          <p class="diagram-caption">Flowchart: classifying a data point by computing its leverage and residual.</p>

          <h3>A Brief History</h3>

          <p>These ideas have a long history. The key milestones:</p>

          <div class="mermaid">
          timeline
            title Leverage and Influence: A Timeline
            1977 : Cook introduces Cook's distance
            1978 : Hoaglin and Welsch formalize the leverage-residual plot
            1986 : Chatterjee and Hadi publish an influential survey
            2006 : Drineas, Mahoney, and Muthukrishnan introduce randomized algorithms for leverage
            2011 : Mahoney publishes a monograph connecting leverage to randomized linear algebra
            2019+ : ML community rediscovers leverage for data valuation, active learning, and uncertainty
          </div>
          <p class="diagram-caption">Nearly five decades of leverage and influence, from classical diagnostics to modern ML.</p>

          <h3>Why ML Forgot</h3>

          <p>The machine learning community largely set aside leverage and influence diagnostics starting in the 1990s. There are understandable reasons:</p>

          <ul>
            <li><strong>Nonlinear models.</strong> The hat matrix is defined for linear regression. Random forests and neural networks do not have one, and many practitioners concluded that leverage was therefore irrelevant to their work.</li>
            <li><strong>High dimensions.</strong> When the number of features $p$ is large relative to the number of data points $n$, every point has high leverage (the average leverage is $p/n$). The concept seems to lose its discriminative power.</li>
            <li><strong>Focus on prediction accuracy.</strong> ML evaluates models on held-out accuracy. Leverage is a training-set diagnostic, which feels like it belongs to a different paradigm.</li>
          </ul>

          <h3>Why It Still Matters</h3>

          <p>Each of these objections has a natural counterargument:</p>

          <ul>
            <li>Leverage measures <strong>data geometry</strong>, which is model-independent. You can compute leverage on the raw feature matrix regardless of whether you fit a linear model, a random forest, or a neural network. The geometry of the data does not change with the model. In practice, even for nonlinear models, the leverage of the input features often correlates well with the difficulty of prediction at that point.</li>
            <li>In high dimensions, leverage <em>variation</em> -- not the average level -- contains useful signal. Even when the average leverage is high, some points have much higher leverage than others, and those differences can be informative about which regions of feature space are well-covered by training data.</li>
            <li>Leverage is not just a training diagnostic. As we showed in Part 4 and will develop further in Part 6, leverage <em>directly controls prediction uncertainty</em>. It is a key ingredient for constructing adaptive prediction intervals.</li>
          </ul>

          <div class="analogy">
            <div class="analogy-label">Analogy</div>
            <p>A leverage point is like a swing voter in a swing state -- their position gives them disproportionate power, whether they use it or not. An influential point is a swing voter in a swing state who actually votes against the majority. The position creates the <em>potential</em> for influence; the unusual outcome <em>activates</em> it.</p>
          </div>

        </div>
      </div>

      <!-- ============================================================
           LEVEL 2: TECHNICAL
           ============================================================ -->
      <div class="level">
        <div class="level-header" role="button" aria-expanded="true" onclick="this.parentElement.querySelector('.level-content').classList.toggle('collapsed'); this.setAttribute('aria-expanded', this.getAttribute('aria-expanded')==='true'?'false':'true');">
          <span class="level-badge technical">Technical</span>
          <h2>Diagnostic Tools: Plots, Distances, and Algorithms</h2>
          <span class="level-toggle" aria-hidden="true">&#9660;</span>
        </div>
        <div class="level-content">

          <h3>The Leverage-Residual Plot (Hoaglin &amp; Welsch, 1978)</h3>

          <p>The leverage-residual plot is a standard tool of regression diagnostics, introduced by Hoaglin and Welsch in 1978 and still widely used today. For each training point $i$, plot the leverage $h_i$ on the horizontal axis against the <strong>standardized residual</strong> on the vertical axis:</p>

          $$r_i = \frac{e_i}{s\sqrt{1 - h_i}}$$

          <p>where $e_i = Y_i - \hat{Y}_i$ is the raw residual and $s^2$ is the estimated residual variance. The denominator $s\sqrt{1-h_i}$ accounts for the fact that high-leverage points tend to have <em>smaller</em> residuals (the model is pulled toward them), so standardizing by $\sqrt{1-h_i}$ puts all residuals on a comparable scale.</p>

          <p>Points in the upper-right corner of this plot are the influential ones: high leverage <em>and</em> large standardized residual. In practice, one typically overlays reference lines at $h = 2p/n$ (twice the average leverage) on the horizontal axis and at $|r_i| = 2$ or $3$ on the vertical axis. Points beyond both thresholds are candidates for further investigation. It is also common to annotate the plot with contours of constant Cook's distance, which curve from upper-left to lower-right; points lying beyond the $D = 4/n$ contour are those whose removal would noticeably shift the fitted model.</p>

          <h3>Cook's Distance: The Product of Two Terms</h3>

          <p>Cook's distance measures the total shift in all fitted values when observation $i$ is deleted:</p>

          $$D_i = \frac{(\hat{\mathbf{Y}} - \hat{\mathbf{Y}}_{(i)})^\top(\hat{\mathbf{Y}} - \hat{\mathbf{Y}}_{(i)})}{p \cdot s^2} = \frac{e_i^2}{p \cdot s^2} \cdot \frac{h_i}{(1-h_i)^2}$$

          <p>The factorization on the right is the key insight. Cook's distance is the product of two terms:</p>

          <div class="mermaid">
          flowchart LR
            CD["Cook's Distance Dᵢ"] --> RT["Residual Term: eᵢ² / (p · s²)"]
            CD --> LT["Leverage Term: hᵢ / (1 − hᵢ)²"]
            RT --> RX["How far is the point from the fitted surface?"]
            LT --> LX["How much power does the point have to move the surface?"]
            RX --> I["Both large => INFLUENTIAL"]
            LX --> I

            style CD fill:#fce4ec,stroke:#c62828,color:#1c1917
            style I fill:#fce4ec,stroke:#c62828,color:#1c1917
          </div>
          <p class="diagram-caption">Cook's distance decomposes into a residual component and a leverage component. Both must be large for true influence.</p>

          <p>A point with a large residual but low leverage produces a moderate Cook's distance -- it is an outlier but not influential. A point with high leverage but a small residual also produces a moderate Cook's distance -- it is a leverage point but the model handles it well. Only the combination of both yields a large Cook's distance. To build concrete intuition: in a dataset with $n = 1000$ and $p = 10$, a point with leverage $h_i = 0.05$ (five times the average) and a standardized residual of $3$ would have a Cook's distance around $0.024$, which is $6\times$ the $4/n = 0.004$ threshold. Removing that single point would shift the entire vector of fitted values by an amount comparable to the residual noise.</p>

          <h3>Practical Flagging Rules</h3>

          <p>The classical literature developed several rules of thumb, still in active use:</p>

          <table>
            <thead>
              <tr>
                <th>Rule</th>
                <th>Criterion</th>
                <th>Interpretation</th>
                <th>Example ($n=1000$, $p=10$)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>High leverage</strong></td>
                <td>$h_i > 2p/n$</td>
                <td>More than twice the average leverage</td>
                <td>$h_i > 0.02$</td>
              </tr>
              <tr>
                <td><strong>Very high leverage</strong></td>
                <td>$h_i > 0.5$</td>
                <td>Large effect on the fit regardless of residual size</td>
                <td>$h_i > 0.5$</td>
              </tr>
              <tr>
                <td><strong>Influential (Cook's)</strong></td>
                <td>$D_i > 4/n$</td>
                <td>Removal shifts predictions substantially</td>
                <td>$D_i > 0.004$</td>
              </tr>
              <tr>
                <td><strong>Highly influential</strong></td>
                <td>$D_i > 1$</td>
                <td>Removal changes the model by more than a joint confidence region</td>
                <td>$D_i > 1$</td>
              </tr>
            </tbody>
          </table>

          <p>These rules remain in every introductory regression course. Yet they are almost completely absent from the machine learning literature, where models are typically evaluated only on held-out prediction accuracy.</p>

          <h3>Randomized SVD for Scalability</h3>

          <p>Computing exact leverage scores via a full SVD of the $n \times p$ design matrix costs $O(np^2)$. For large-scale datasets, this becomes prohibitive. Randomized algorithms provide a way out.</p>

          <p>The approach, developed by Drineas, Mahoney, and Muthukrishnan (2006, 2012), sketches the design matrix $\mathbf{X}$ using a random projection, computes the SVD of the (much smaller) sketch, and uses it to approximate leverage scores. The result is $(1+\varepsilon)$-approximate leverage scores in time $O(np\log p)$.</p>

          <div class="mermaid">
          flowchart LR
            subgraph Exact ["Exact SVD"]
              direction TB
              E1["Full SVD of X (n × p)"] --> E2["Cost: O(np²)"]
              E2 --> E3["Exact leverage scores"]
            end

            subgraph Rand ["Randomized SVD"]
              direction TB
              R1["Random projection of X"] --> R2["SVD of sketch"]
              R2 --> R3["Cost: O(np log p)"]
              R3 --> R4["(1+ε)-approximate leverage"]
            end

            subgraph CW ["Clarkson-Woodruff"]
              direction TB
              C1["Sparse random projection"] --> C2["Exploit sparsity in X"]
              C2 --> C3["Cost: O(nnz(X) + p³)"]
              C3 --> C4["Input-sparsity time"]
            end

            style E2 fill:#fce4ec,stroke:#c62828,color:#1c1917
            style R3 fill:#e3f2fd,stroke:#1565c0,color:#1c1917
            style C3 fill:#e8f5e9,stroke:#2e7d32,color:#1c1917
          </div>
          <p class="diagram-caption">Computational cost comparison: exact SVD, randomized SVD, and Clarkson-Woodruff. For large sparse datasets, input-sparsity time is nearly linear.</p>

          <p>These algorithmic advances mean that leverage scores are computable at the scale of modern datasets. The cost is comparable to -- or less than -- a single pass over the data.</p>

          <h3>Leverage Beyond Linear Regression</h3>

          <p>Although leverage is defined via the hat matrix of a linear model, the concept extends naturally to other settings:</p>

          <p><strong>Ridge regression.</strong> When $p > n$ or the design matrix is ill-conditioned, OLS leverage is not well-defined. Ridge regression replaces $(\mathbf{X}^\top\mathbf{X})^{-1}$ with $(\mathbf{X}^\top\mathbf{X} + \lambda\mathbf{I})^{-1}$, giving <em>ridge leverage scores</em>:</p>

          $$h_\lambda(x) = x^\top (\mathbf{X}^\top \mathbf{X} + \lambda \mathbf{I})^{-1} x$$

          <p>Ridge leverage scores are always well-defined and bounded above by $1/\lambda$. As $\lambda \to 0$, they approach OLS leverage; as $\lambda \to \infty$, they shrink toward zero.</p>

          <p><strong>Kernel methods.</strong> In kernel regression or kernel ridge regression, the design matrix lives in a (possibly infinite-dimensional) feature space. Leverage is defined in terms of the kernel matrix:</p>

          $$h_i = (\mathbf{K}(\mathbf{K} + \lambda\mathbf{I})^{-1})_{ii}$$

          <p><strong>Neural networks.</strong> For a neural network, one can define <em>feature-space leverage</em> using the activations of the penultimate layer. If $\Phi(x) \in \mathbb{R}^d$ denotes the penultimate-layer representation, then:</p>

          $$h^{NN}(x) = \Phi(x)^\top (\boldsymbol{\Phi}^\top \boldsymbol{\Phi} + \lambda \mathbf{I})^{-1} \Phi(x)$$

          <p>This measures how unusual $x$ is in the <em>learned representation space</em>, which may be more relevant than the raw feature space. In all cases, the interpretation is the same: leverage measures how unusual a point is relative to the training distribution, in a geometry adapted to the model's representation.</p>

        </div>
      </div>

      <!-- ============================================================
           LEVEL 3: ADVANCED
           ============================================================ -->
      <div class="level">
        <div class="level-header" role="button" aria-expanded="true" onclick="this.parentElement.querySelector('.level-content').classList.toggle('collapsed'); this.setAttribute('aria-expanded', this.getAttribute('aria-expanded')==='true'?'false':'true');">
          <span class="level-badge advanced">Advanced</span>
          <h2>Derivations: Cook's Distance, Sherman-Morrison, and Sketching</h2>
          <span class="level-toggle" aria-hidden="true">&#9660;</span>
        </div>
        <div class="level-content">

          <h3>Full Derivation of Cook's Distance</h3>

          <p>We derive Cook's distance from scratch using the leave-one-out formula. The key tool is the Sherman-Morrison-Woodbury identity, which allows us to express the leave-one-out estimator $\hat{\beta}_{(i)}$ in closed form without refitting.</p>

          <h4>Step 1: The Leave-One-Out Coefficient Update</h4>

          <p>Start with the OLS estimator $\hat{\beta} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{Y}$. When we delete observation $i$, the design matrix becomes $\mathbf{X}_{(i)}$ (with row $i$ removed) and the response becomes $\mathbf{Y}_{(i)}$. The leave-one-out estimator is:</p>

          $$\hat{\beta}_{(i)} = (\mathbf{X}_{(i)}^\top\mathbf{X}_{(i)})^{-1}\mathbf{X}_{(i)}^\top\mathbf{Y}_{(i)}$$

          <p>The Sherman-Morrison identity gives us a way to invert the rank-one-updated matrix:</p>

          $$(\mathbf{X}_{(i)}^\top\mathbf{X}_{(i)})^{-1} = (\mathbf{X}^\top\mathbf{X} - X_i X_i^\top)^{-1} = (\mathbf{X}^\top\mathbf{X})^{-1} + \frac{(\mathbf{X}^\top\mathbf{X})^{-1}X_i X_i^\top(\mathbf{X}^\top\mathbf{X})^{-1}}{1 - h_i}$$

          <p>After substitution and simplification, the leave-one-out coefficient vector becomes:</p>

          $$\hat{\beta}_{(i)} = \hat{\beta} - \frac{(\mathbf{X}^\top\mathbf{X})^{-1}X_i e_i}{1 - h_i}$$

          <p>where $e_i = Y_i - X_i^\top\hat{\beta}$ is the raw residual. This formula is worth pausing on: the leave-one-out coefficient change is proportional to the residual $e_i$, amplified by the leverage factor $1/(1-h_i)$, and directed along $(\mathbf{X}^\top\mathbf{X})^{-1}X_i$.</p>

          <div class="mermaid">
          flowchart TD
            A["Full model: β̂ = (XᵀX)⁻¹XᵀY"] --> B["Delete observation i"]
            B --> C["Sherman-Morrison identity on (XᵀX − xᵢxᵢᵀ)⁻¹"]
            C --> D["Leave-one-out: β̂₍ᵢ₎ = β̂ − (XᵀX)⁻¹xᵢeᵢ / (1 − hᵢ)"]
            D --> E["Prediction shift: Ŷⱼ − Ŷⱼ₍ᵢ₎ = Hⱼᵢeᵢ / (1 − hᵢ)"]
            E --> F["Sum of squared shifts: Σⱼ (Hⱼᵢeᵢ / (1−hᵢ))²"]
            F --> G["= eᵢ²hᵢ / (1 − hᵢ)²"]
            G --> H["Cook's distance: Dᵢ = eᵢ²hᵢ / (p·s²·(1−hᵢ)²)"]

            style A fill:#e3f2fd,stroke:#1565c0,color:#1c1917
            style D fill:#fff3e0,stroke:#e65100,color:#1c1917
            style H fill:#fce4ec,stroke:#c62828,color:#1c1917
          </div>
          <p class="diagram-caption">The logical chain from the full model to Cook's distance, via Sherman-Morrison.</p>

          <h4>Step 2: The Prediction Shift</h4>

          <p>The change in the predicted value for observation $j$ when observation $i$ is deleted is:</p>

          $$\hat{Y}_j - \hat{Y}_{j,(i)} = X_j^\top(\hat{\beta} - \hat{\beta}_{(i)}) = X_j^\top \frac{(\mathbf{X}^\top\mathbf{X})^{-1}X_i e_i}{1-h_i} = \frac{H_{ji} \cdot e_i}{1 - h_i}$$

          <p>where $H_{ji} = X_j^\top(\mathbf{X}^\top\mathbf{X})^{-1}X_i$ is the $(j,i)$ entry of the hat matrix.</p>

          <h4>Step 3: Summing the Squared Shifts</h4>

          <p>Cook's distance sums the squared prediction shifts over all observations, normalized by $p \cdot s^2$:</p>

          $$D_i = \frac{1}{p \cdot s^2} \sum_{j=1}^n \left(\hat{Y}_j - \hat{Y}_{j,(i)}\right)^2 = \frac{1}{p \cdot s^2} \sum_{j=1}^n \frac{H_{ji}^2 \cdot e_i^2}{(1-h_i)^2}$$

          <p>The sum $\sum_j H_{ji}^2$ simplifies because $\mathbf{H}$ is symmetric and idempotent: $\sum_j H_{ji}^2 = (\mathbf{H}^2)_{ii} = H_{ii} = h_i$. Therefore:</p>

          $$D_i = \frac{e_i^2}{p \cdot s^2} \cdot \frac{h_i}{(1-h_i)^2}$$

          <p>This completes the derivation. The factorization into a residual term and a leverage term emerges naturally from the algebra.</p>

          <h3>Ridge Leverage and Its Bounds</h3>

          <p>When the design matrix is ill-conditioned or $p > n$, the OLS hat matrix is not well-defined. Ridge regression introduces a regularization parameter $\lambda > 0$, which modifies the leverage score to:</p>

          $$h_\lambda(x) = x^\top(\mathbf{X}^\top\mathbf{X} + \lambda\mathbf{I})^{-1}x$$

          <p>To see that ridge leverage is bounded by $1/\lambda$, note that $\mathbf{X}^\top\mathbf{X} + \lambda\mathbf{I} \succeq \lambda\mathbf{I}$, so $(\mathbf{X}^\top\mathbf{X} + \lambda\mathbf{I})^{-1} \preceq \lambda^{-1}\mathbf{I}$, and therefore:</p>

          $$h_\lambda(x) = x^\top(\mathbf{X}^\top\mathbf{X} + \lambda\mathbf{I})^{-1}x \leq \frac{\|x\|^2}{\lambda}$$

          <p>In terms of the SVD $\mathbf{X} = \mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^\top$, the ridge leverage scores of the training data are:</p>

          $$h_{\lambda,i} = \sum_{k=1}^p \frac{\sigma_k^2}{\sigma_k^2 + \lambda} U_{ik}^2$$

          <p>Each singular component is shrunk by the factor $\sigma_k^2/(\sigma_k^2 + \lambda)$, which is close to 1 for large singular values and close to 0 for small ones. Ridge leverage effectively discounts directions in which the data have little variance.</p>

          <h3>Kernel Leverage</h3>

          <p>In kernel methods, we map data to a (possibly infinite-dimensional) feature space via a kernel function $k(\cdot, \cdot)$. The kernel matrix is $\mathbf{K} \in \mathbb{R}^{n \times n}$ with $K_{ij} = k(X_i, X_j)$. The kernel ridge leverage scores are:</p>

          $$h_i = (\mathbf{K}(\mathbf{K} + \lambda\mathbf{I})^{-1})_{ii}$$

          <p>This can be expressed via the eigendecomposition of $\mathbf{K} = \mathbf{Q}\boldsymbol{\Lambda}\mathbf{Q}^\top$ as:</p>

          $$h_i = \sum_{k=1}^n \frac{\lambda_k}{\lambda_k + \lambda} Q_{ik}^2$$

          <p>The effective dimensionality of the kernel feature space is captured by the sum $\sum_k \lambda_k/(\lambda_k + \lambda)$, which plays the role of $p$ in the linear case.</p>

          <h3>Neural Network Leverage</h3>

          <p>For a neural network with a final linear layer on top of a learned representation $\Phi(x) \in \mathbb{R}^d$, the leverage score is:</p>

          $$h^{NN}(x) = \Phi(x)^\top(\boldsymbol{\Phi}^\top\boldsymbol{\Phi} + \lambda\mathbf{I})^{-1}\Phi(x)$$

          <p>where $\boldsymbol{\Phi} \in \mathbb{R}^{n \times d}$ is the matrix of training representations. This is mathematically identical to ridge leverage, but computed in the <em>learned</em> feature space rather than the raw input space. The key distinction is that $\Phi$ depends on the model parameters, so the leverage scores reflect the geometry that the network has learned to be relevant for the prediction task.</p>

          <h3>Randomized Leverage Computation</h3>

          <p>For very large matrices, even $O(np\log p)$ may be expensive. The Clarkson-Woodruff (2013) sketch uses a sparse random sign matrix $\mathbf{S} \in \mathbb{R}^{m \times n}$ (with exactly one nonzero entry per column) to compute $\mathbf{S}\mathbf{X}$ in $O(\text{nnz}(\mathbf{X}))$ time -- the number of nonzero entries, which is the minimum possible for reading the matrix.</p>

          <p>The algorithm proceeds as follows:</p>

          <ol>
            <li>Compute the sketch $\mathbf{Y} = \mathbf{S}\mathbf{X}$ in $O(\text{nnz}(\mathbf{X}))$ time.</li>
            <li>Compute a QR factorization $\mathbf{Y} = \mathbf{Q}_Y\mathbf{R}$ in $O(mp^2)$ time.</li>
            <li>Form $\mathbf{Z} = \mathbf{X}\mathbf{R}^{-1}$ in $O(np^2)$ time.</li>
            <li>The approximate leverage scores are $\tilde{h}_i = \|z_i\|^2$ where $z_i$ is the $i$-th row of $\mathbf{Z}$.</li>
          </ol>

          <p>With $m = O(p^2/\varepsilon^2)$, this gives $(1+\varepsilon)$-multiplicative approximations. For sparse matrices where $\text{nnz}(\mathbf{X}) \ll np$, the total cost is $O(\text{nnz}(\mathbf{X}) + p^3)$ -- the input-sparsity time result of Clarkson and Woodruff.</p>

          <div class="callout">
            <div class="callout-label">Key Insight</div>
            <p>The ability to approximate leverage scores in nearly input-sparsity time means that these classical diagnostics scale to modern dataset sizes. Computing leverage is comparable in cost to reading the data, which substantially weakens the "computational cost" objection to leverage-based methods in ML.</p>
          </div>

        </div>
      </div>

      <!-- ============================================================
           LOOKING AHEAD
           ============================================================ -->

      <h2>The Deeper Story</h2>

      <p>We have presented leverage and influence as diagnostic tools: ways to find unusual, potentially problematic data points. This is their classical role, and it is a valuable one.</p>

      <p>But leverage has a much deeper connection to prediction uncertainty. In Part 4, we previewed the formula $\text{Var}(Y_{\text{new}} - \hat{Y}(x)) = \sigma^2(1 + h(x))$. This says that the variance of the prediction error -- the very quantity that prediction intervals need to capture -- is directly controlled by leverage.</p>

      <p>In the next post, we examine this connection in detail and identify a subtlety that is easy to miss: the variance of <em>training residuals</em> depends on leverage with the <em>opposite sign</em> from the variance of <em>prediction errors</em>. This "sign flip" has practical consequences for any method that tries to estimate prediction uncertainty from training data.</p>

    </div>

    <div class="further-reading">
      <h3>Further Reading</h3>
      <ul>
        <li>Hoaglin, D. C. &amp; Welsch, R. E. (1978). The hat matrix in regression and ANOVA. <em>The American Statistician</em>, 32(1), 17-22.</li>
        <li>Chatterjee, S. &amp; Hadi, A. S. (1986). Influential observations, high leverage points, and outliers in linear regression. <em>Statistical Science</em>, 1(3), 379-393.</li>
        <li>Drineas, P., Mahoney, M. W., &amp; Muthukrishnan, S. (2006). Sampling algorithms for $\ell_2$ regression and applications. <em>Proceedings of the 17th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)</em>, 1127-1136.</li>
        <li>Mahoney, M. W. (2011). Randomized algorithms for matrices and data. <em>Foundations and Trends in Machine Learning</em>, 3(2), 123-224.</li>
      </ul>
    </div>

    <div class="post-nav">
      <a href="post04.html" class="prev">The Hat Matrix</a>
      <a href="post06.html" class="next">The Sign Flip</a>
    </div>

  </article>
</main>

<footer>
  <span>Shreyas Fadnavis</span>
  <span><a href="/blog/">Notes</a></span>
</footer>

<script src="../js/lightbox.js"></script>
</body>
</html>
